{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project - Text Analysis,Sentimental Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_file = \"C:\\\\Users\\\\Dell\\\\Desktop\\\\Sentiment Analysis\\\\StopWords\\\\StopWords_Auditor.txt\"\n",
    "stop_words = set()\n",
    "\n",
    "with open(stop_words_file, 'r') as f:\n",
    "    stop_words.update(f.read().lower().splitlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UnicodeDecodeError for file: StopWords_Currencies.txt, retrying with ISO-8859-1.\n",
      "Loaded 12768 stopwords.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def load_stopwords_from_files(stop_words_dir):\n",
    "    stop_words = set()\n",
    "    \n",
    "    # Loop through all entries in the directory\n",
    "    for file_name in os.listdir(stop_words_dir):\n",
    "        file_path = os.path.join(stop_words_dir, file_name)\n",
    "        \n",
    "        # Check if the entry is a file and ends with .txt\n",
    "        if os.path.isfile(file_path) and file_name.endswith('.txt'):\n",
    "            try:\n",
    "                # Try utf-8 and fallback to ISO-8859-1\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    stop_words.update(file.read().lower().splitlines())\n",
    "            except UnicodeDecodeError:\n",
    "                print(f\"UnicodeDecodeError for file: {file_name}, retrying with ISO-8859-1.\")\n",
    "                with open(file_path, 'r', encoding='ISO-8859-1') as file:\n",
    "                    stop_words.update(file.read().lower().splitlines())\n",
    "    \n",
    "    return stop_words\n",
    "\n",
    "# Specify the directory containing stopword files\n",
    "stop_words_dir = \"C:\\\\Users\\\\Dell\\\\Desktop\\\\Sentiment Analysis\\\\StopWords\"\n",
    "\n",
    "# Load stopwords\n",
    "stop_words = load_stopwords_from_files(stop_words_dir)\n",
    "\n",
    "# Print a sample to confirm (Optional for debugging)\n",
    "print(f\"Loaded {len(stop_words)} stopwords.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2006 positive words.\n",
      "Loaded 4783 negative words.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "master_dict_dir = \"MasterDictionary\"\n",
    "positive_words_file = os.path.join(master_dict_dir, \"C:\\\\Users\\\\Dell\\\\Desktop\\\\Sentiment Analysis\\\\MasterDictionary\\\\positive-words.txt\")\n",
    "negative_words_file = os.path.join(master_dict_dir, \"C:\\\\Users\\\\Dell\\\\Desktop\\\\Sentiment Analysis\\\\MasterDictionary\\\\negative-words.txt\")\n",
    "\n",
    "positive_words = set()\n",
    "negative_words = set()\n",
    "\n",
    "# Load Positive Words\n",
    "with open(positive_words_file, 'r') as f:\n",
    "    positive_words.update(f.read().lower().splitlines())\n",
    "\n",
    "# Load Negative Words\n",
    "with open(negative_words_file, 'r') as f:\n",
    "    negative_words.update(f.read().lower().splitlines())\n",
    "\n",
    "# Print Loaded Words (Optional for Debugging)\n",
    "print(f\"Loaded {len(positive_words)} positive words.\")\n",
    "print(f\"Loaded {len(negative_words)} negative words.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(master_dict_dir, \"positive-words.txt\"), 'r') as f:\n",
    "    positive_words.update(f.read().lower().splitlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(master_dict_dir, \"negative-words.txt\"), 'r') as f:\n",
    "    negative_words.update(f.read().lower().splitlines())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Cleaning and Tokenization\n",
    "def clean_and_tokenize(text):\n",
    "    text = re.sub(r'[\\W_]+', ' ', text).lower()\n",
    "    words = word_tokenize(text)\n",
    "    cleaned_words = [word for word in words if word not in stop_words]\n",
    "    return cleaned_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derived Variable Calculation\n",
    "def calculate_scores(text):\n",
    "    words = clean_and_tokenize(text)\n",
    "    positive_score = sum(1 for word in words if word in positive_words)\n",
    "    negative_score = sum(1 for word in words if word in negative_words)\n",
    "    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 1e-6)\n",
    "    subjectivity_score = (positive_score + negative_score) / (len(words) + 1e-6)\n",
    "    return positive_score, negative_score, polarity_score, subjectivity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Readability Analysis\n",
    "def readability_analysis(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    words = clean_and_tokenize(text)\n",
    "    word_count = len(words)\n",
    "    sentence_count = len(sentences)\n",
    "    \n",
    "    avg_sentence_length = word_count / sentence_count if sentence_count else 0\n",
    "\n",
    "    def count_syllables(word):\n",
    "        word = word.lower()\n",
    "        vowels = \"aeiou\"\n",
    "        syllables = sum(1 for char in word if char in vowels)\n",
    "        if word.endswith(('es', 'ed')):\n",
    "            syllables = max(1, syllables - 1)\n",
    "        return syllables\n",
    "    complex_words = [word for word in words if count_syllables(word) > 2]\n",
    "    percentage_complex_words = len(complex_words) / word_count if word_count else 0\n",
    "\n",
    "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
    "    return avg_sentence_length, percentage_complex_words, fog_index    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other Metrics\n",
    "def additional_metrics(text):\n",
    "    words = clean_and_tokenize(text)\n",
    "    total_chars = sum(len(word) for word in words)\n",
    "    avg_word_length = total_chars / len(words) if words else 0\n",
    "\n",
    "    personal_pronouns = len(re.findall(r\"\\b(i|we|my|ours|us)\\b\", text, re.IGNORECASE))\n",
    "\n",
    "    syllable_count_per_word = [count_syllables(word) for word in words]\n",
    "\n",
    "    return len(words), avg_word_length, personal_pronouns, syllable_count_per_word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Score: 0\n",
      "Negative Score: 0\n",
      "Polarity Score: 0.0\n",
      "Subjectivity Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    sample_text = \"Your sample financial text goes here.\"\n",
    "\n",
    "    # Derived Variables\n",
    "    pos_score, neg_score, polarity, subjectivity = calculate_scores(sample_text)\n",
    "    print(f\"Positive Score: {pos_score}\")\n",
    "    print(f\"Negative Score: {neg_score}\")\n",
    "    print(f\"Polarity Score: {polarity}\")\n",
    "    print(f\"Subjectivity Score: {subjectivity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of Readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readability_analysis(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    words = clean_and_tokenize(text)\n",
    "    word_count = len(words)\n",
    "    sentence_count = len(sentences)\n",
    "    \n",
    "    avg_sentence_length = word_count / sentence_count if sentence_count else 0\n",
    "\n",
    "    def count_syllables(word):\n",
    "        word = word.lower()\n",
    "        vowels = \"aeiou\"\n",
    "        syllables = sum(1 for char in word if char in vowels)\n",
    "        if word.endswith(('es', 'ed')):\n",
    "            syllables = max(1, syllables - 1)\n",
    "        return syllables\n",
    "\n",
    "    complex_words = [word for word in words if count_syllables(word) > 2]\n",
    "    percentage_complex_words = len(complex_words) / word_count if word_count else 0\n",
    "\n",
    "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
    "    return avg_sentence_length, percentage_complex_words, fog_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Sentence Length: 1.0\n",
      "Percentage of Complex Words: 0.0\n",
      "Fog Index: 0.4\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"Your sample text goes here.\"\n",
    "avg_sentence_length, perc_complex_words, fog_index = readability_analysis(sample_text)\n",
    "\n",
    "print(f\"Average Sentence Length: {avg_sentence_length}\")\n",
    "print(f\"Percentage of Complex Words: {perc_complex_words}\")\n",
    "print(f\"Fog Index: {fog_index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional Metrics\n",
    "def additional_metrics(text):\n",
    "    words = clean_and_tokenize(text)\n",
    "    total_chars = sum(len(word) for word in words)\n",
    "    avg_word_length = total_chars / len(words) if words else 0\n",
    "\n",
    "    personal_pronouns = len(re.findall(r\"\\\\b(i|we|my|ours|us)\\\\b\", text, re.IGNORECASE))\n",
    "\n",
    "    def count_syllables(word):\n",
    "        word = word.lower()\n",
    "        vowels = \"aeiou\"\n",
    "        syllables = sum(1 for char in word if char in vowels)\n",
    "        if word.endswith(('es', 'ed')):\n",
    "            syllables = max(1, syllables - 1)\n",
    "        return syllables\n",
    "\n",
    "    syllable_count_per_word = [count_syllables(word) for word in words]\n",
    "\n",
    "    return len(words), avg_word_length, personal_pronouns, syllable_count_per_word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Count: 1\n",
      "Average Word Length: 4.0\n",
      "Personal Pronouns: 0\n",
      "Syllable Counts: [1]\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"Your sample text goes here.\"\n",
    "word_count, avg_word_length, personal_pronouns, syllable_counts = additional_metrics(sample_text)\n",
    "\n",
    "print(f\"Word Count: {word_count}\")\n",
    "print(f\"Average Word Length: {avg_word_length}\")\n",
    "print(f\"Personal Pronouns: {personal_pronouns}\")\n",
    "print(f\"Syllable Counts: {syllable_counts}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
